{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3\n",
    "### Steps to build a decision tree\n",
    "1. 计算当前集合S中数据的熵$H(S)=-\\sum_{i=1}^m p_i\\log(p_i)$。其中m为feature的数目。\n",
    "2. 遍历没有被split的feature们，计算按各个feature进行split后的熵增益$IG_{A, S}=H(S) - \\sum_{i=1}^{K_A} p_{sub_i}H(sub_i)$。\n",
    "3. 选择熵增益最大的feature进行split，并生成对应的树节点。对分成的子集合递归调用step 1。\n",
    "\n",
    "\n",
    "## CART\n",
    "CART stands for Classification And Regression Trees. \n",
    "### 如何进行split的测试\n",
    "每次split都是二分的，针对不同的数值情况有：\n",
    "1. 类别的数据，从所有类别中选择所有的可能的划分，除了空集。\n",
    "2. 数值的数据，把所有值从小到大排序，选择所有$a=\\frac{(x_k+x_{k+1})}{2}$来进行划分。\n",
    "3. 顺序的数据，选择$x_{min} < a < x_{max}$来进行划分。\n",
    "\n",
    "### Gini impurity （用于分类）\n",
    "$$I_G(f)=\\sum_{i=1}^m f_i(1-f_i)=1-\\sum_{i=1}^m f_i^2$$\n",
    "Gini impurity的差值被拿来进行split，使用的思路和ID3中的IG类似。\n",
    "\n",
    "### Squared error （用于regression）\n",
    "$$\\sum(y-E(y))^2$$\n",
    "求$p_l{\\sum(y_l-E(y_l))^2}+p_r{\\sum(y_r-E(y_r))^2}$的最小划分。\n",
    "\n",
    "## Advantages of Decision Tree\n",
    "* 容易理解和实现\n",
    "* 可以处理连续和离散的数据\n",
    "* 可以处理多分类问题\n",
    "\n",
    "## Disadvantages of Decision Tree\n",
    "* 容易overfitting\n",
    "* 有可能不稳定，小的数据变化会造成完全不同的树\n",
    "* 数据分布不是很均匀的话，容易导致high bias。\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
